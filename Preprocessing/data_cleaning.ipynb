{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "\n",
    "# pre-preocessing utilties\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base path\n",
    "base_path  = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "# Set file path\n",
    "file_path = base_path + '\\\\RawDatasets\\\\'\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data set\n",
    "df = pd.read_csv(file_path + 'tweets_india.csv')\n",
    "df.columns = ['id', 'date', 'content']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Some information about the data set\n",
    "print('Length of data set: ', len(df))\n",
    "print('Shape of data set: ', df.shape)\n",
    "print('Dataset information')\n",
    "print('-------------------')\n",
    "df.info()\n",
    "\n",
    "# Check for null values - needs to be zero\n",
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Converting all text to lower case\n",
    "def convert_to_lower_case(data_set):\n",
    "    data_set['content'] = data_set['content'].str.lower()\n",
    "\n",
    "convert_to_lower_case(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removing stop words (un-necessary words) - using nltk's pre-defined stop words\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "print('Stop word list:')\n",
    "print('----------------')\n",
    "print(STOP_WORDS)\n",
    "\n",
    "def remove_stop_words(content):\n",
    "   return \" \".join([text for text in str(content).split() if text not in STOP_WORDS])\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_stop_words(content=content))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Removing URLs\n",
    "\n",
    "def remove_URLS(content):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_URLS(content=content))\n",
    "df['content'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Removing @mentions\n",
    "\n",
    "def remove_mentions(content):\n",
    "    return re.sub('(@\\S+)',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_mentions(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Removing numbers\n",
    "\n",
    "def remove_numericals(content):\n",
    "        return re.sub('[0-9]+', '', content)\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_numericals(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Removing punctuations\n",
    "PUNCTUATIONS = string.punctuation\n",
    "print('Punctuation list:')\n",
    "print('------------------')\n",
    "print(PUNCTUATIONS)\n",
    "print('------------------')\n",
    "\n",
    "def remove_punctuations(content):\n",
    "    return str(content).translate(str.maketrans('', '', PUNCTUATIONS))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_punctuations(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unicodes\n",
    "def remove_unicodes(content):\n",
    "        return str(content).encode('ascii', errors='ignore').decode()\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_unicodes(content=content))\n",
    "df['content'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Remove unwanted emojis\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: clean(str(content), no_emoji=True))\n",
    "df['content'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Tokenizing the texts\n",
    "df['content'] = df['content'].apply(lambda content: word_tokenize(str(content)))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Stemming of the words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stemming_content(content):\n",
    "    text = [stemmer.stem(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content']= df['content'].apply(lambda content: stemming_content(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Lemmatizing the tokens\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "def lemmatizing_content(content):\n",
    "    text = [lemmatizer.lemmatize(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: lemmatizing_content(content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing pre-processed data into csv\n",
    "output_file_path = 'd:\\\\Varshini\\\\CourseWork\\\\Machine Learning\\\\project\\\\MachineLearning-group-project\\\\Preprocessing'\n",
    "df.to_csv('tweets_india_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('mlenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1306746bd56c4df6af1e05061875d5a1c518f29945151cd9f83e4556169c61d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
