{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sureshv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sureshv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sureshv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sureshv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# pre-preocessing utilties\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path\n",
    "file_path = 'd:\\\\Varshini\\\\CourseWork\\\\Machine Learning\\\\project\\\\MachineLearning-group-project\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                       date  \\\n",
      "0   0  2022-09-09 20:45:28+00:00   \n",
      "1   1  2022-09-09 20:41:19+00:00   \n",
      "2   2  2022-09-09 19:25:57+00:00   \n",
      "3   3  2022-09-09 19:14:54+00:00   \n",
      "4   4  2022-09-09 19:13:38+00:00   \n",
      "\n",
      "                                             content  \n",
      "0  Have a nice night trip with T \\r\\nUK to Delhi....  \n",
      "1  May her soul RIP. As a nation we are with the ...  \n",
      "2  @SanjayAzadSln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...  \n",
      "3  @firki07 Firki with queen üò≠ https://t.co/mhvgH...  \n",
      "4  @TuckerCarlson This so called great Britain lo...  \n",
      "Length of data set:  932\n",
      "Shape of data set:  (932, 3)\n",
      "Dataset information\n",
      "-------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 932 entries, 0 to 931\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       932 non-null    int64 \n",
      " 1   date     932 non-null    object\n",
      " 2   content  932 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 22.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set\n",
    "df = pd.read_csv(file_path + 'tweets_india.csv')\n",
    "df.columns = ['id', 'date', 'content']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Some information about the data set\n",
    "print('Length of data set: ', len(df))\n",
    "print('Shape of data set: ', df.shape)\n",
    "print('Dataset information')\n",
    "print('-------------------')\n",
    "df.info()\n",
    "\n",
    "# Check for null values - needs to be zero\n",
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-09 20:45:28+00:00</td>\n",
       "      <td>have a nice night trip with t \\r\\nuk to delhi....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-09 20:41:19+00:00</td>\n",
       "      <td>may her soul rip. as a nation we are with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-09 19:25:57+00:00</td>\n",
       "      <td>@sanjayazadsln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-09 19:14:54+00:00</td>\n",
       "      <td>@firki07 firki with queen üò≠ https://t.co/mhvgh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-09 19:13:38+00:00</td>\n",
       "      <td>@tuckercarlson this so called great britain lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       date  \\\n",
       "0   0  2022-09-09 20:45:28+00:00   \n",
       "1   1  2022-09-09 20:41:19+00:00   \n",
       "2   2  2022-09-09 19:25:57+00:00   \n",
       "3   3  2022-09-09 19:14:54+00:00   \n",
       "4   4  2022-09-09 19:13:38+00:00   \n",
       "\n",
       "                                             content  \n",
       "0  have a nice night trip with t \\r\\nuk to delhi....  \n",
       "1  may her soul rip. as a nation we are with the ...  \n",
       "2  @sanjayazadsln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...  \n",
       "3  @firki07 firki with queen üò≠ https://t.co/mhvgh...  \n",
       "4  @tuckercarlson this so called great britain lo...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Converting all text to lower case\n",
    "def convert_to_lower_case(data_set):\n",
    "    data_set['content'] = data_set['content'].str.lower()\n",
    "\n",
    "convert_to_lower_case(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop word list:\n",
      "----------------\n",
      "{'only', 'because', 'few', 'their', 'further', 'is', 'its', 'an', 'up', 'aren', 'herself', 'themselves', 'who', 'yours', 'here', 'shan', 's', 'or', 'yourselves', \"you're\", \"that'll\", \"didn't\", 'she', \"doesn't\", 'which', 'mustn', 'all', 'him', 'the', \"you've\", 'been', 'did', \"you'll\", 'against', 'will', \"mightn't\", 'couldn', 'yourself', 'you', 'not', 'over', \"should've\", 'those', \"won't\", 'it', 'until', \"wouldn't\", 'again', 'between', 'after', 'don', 'myself', 'these', 'll', 'needn', 'off', 'for', 'from', 'hers', \"you'd\", 'through', 'do', 'hasn', 'own', 'as', 'be', 'into', 'why', \"aren't\", \"hadn't\", 'her', \"wasn't\", 'below', 'me', 'other', 'during', 'once', 'them', 'he', 'this', 'mightn', 'down', 'same', \"shouldn't\", 'such', 'when', 'just', 'has', 'about', 'your', 'no', 'having', 'too', 'wasn', 'am', 'didn', 'his', 'then', 'haven', 'ourselves', 'had', 'while', 'they', 'to', 'y', 'by', \"it's\", 'so', 'each', 'are', \"don't\", 've', 'out', \"needn't\", 'being', 'i', 'now', 'my', \"couldn't\", 'a', 'were', 'there', \"isn't\", 'o', 'that', \"hasn't\", 'whom', 'what', 'and', 'our', \"she's\", 'd', 'any', 'most', 'where', 'wouldn', 'm', 't', 'hadn', 'above', 'himself', 'can', 'theirs', 'how', \"shan't\", 'but', 'does', 'weren', 're', 'itself', 'in', 'we', 'ours', 'more', 'very', 'with', 'than', 'doesn', 'have', 'isn', 'at', 'won', 'doing', 'both', 'nor', \"mustn't\", 'shouldn', 'on', \"weren't\", 'ain', 'under', 'should', 'before', 'ma', 'of', 'some', \"haven't\", 'was', 'if'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-09 20:45:28+00:00</td>\n",
       "      <td>nice night trip uk delhi. üòçüòçüòç https://t.co/7am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-09 20:41:19+00:00</td>\n",
       "      <td>may soul rip. nation family fellow uk citizens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-09 19:25:57+00:00</td>\n",
       "      <td>@sanjayazadsln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-09 19:14:54+00:00</td>\n",
       "      <td>@firki07 firki queen üò≠ https://t.co/mhvgh7aqsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-09 19:13:38+00:00</td>\n",
       "      <td>@tuckercarlson called great britain looted 45 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       date  \\\n",
       "0   0  2022-09-09 20:45:28+00:00   \n",
       "1   1  2022-09-09 20:41:19+00:00   \n",
       "2   2  2022-09-09 19:25:57+00:00   \n",
       "3   3  2022-09-09 19:14:54+00:00   \n",
       "4   4  2022-09-09 19:13:38+00:00   \n",
       "\n",
       "                                             content  \n",
       "0  nice night trip uk delhi. üòçüòçüòç https://t.co/7am...  \n",
       "1  may soul rip. nation family fellow uk citizens...  \n",
       "2  @sanjayazadsln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...  \n",
       "3     @firki07 firki queen üò≠ https://t.co/mhvgh7aqsc  \n",
       "4  @tuckercarlson called great britain looted 45 ...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Removing stop words (un-necessary words) - using nltk's pre-defined stop words\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "print('Stop word list:')\n",
    "print('----------------')\n",
    "print(STOP_WORDS)\n",
    "\n",
    "def remove_stop_words(content):\n",
    "   return \" \".join([text for text in str(content).split() if text not in STOP_WORDS])\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_stop_words(content=content))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      nice night trip uk delhi. üòçüòçüòç  \n",
       "1    may soul rip. nation family fellow uk citizens...\n",
       "2    @sanjayazadsln ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞...\n",
       "3                           @firki07 firki queen üò≠  sc\n",
       "4    @tuckercarlson called great britain looted 45 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Removing URLs\n",
    "\n",
    "def remove_URLS(content):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_URLS(content=content))\n",
    "df['content'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      nice night trip uk delhi. üòçüòçüòç  \n",
       "1    may soul rip. nation family fellow uk citizens...\n",
       "2      ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞‡•Ä ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â...\n",
       "3                                    firki queen üò≠  sc\n",
       "4      called great britain looted 45 trillion usd ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Removing @mentions\n",
    "\n",
    "def remove_mentions(content):\n",
    "    return re.sub('(@\\S+)',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_mentions(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      nice night trip uk delhi. üòçüòçüòç  \n",
       "1    may soul rip. nation family fellow uk citizens...\n",
       "2      ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞‡•Ä ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â...\n",
       "3                                    firki queen üò≠  sc\n",
       "4      called great britain looted  trillion usd in...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Removing numbers\n",
    "\n",
    "def remove_numericals(content):\n",
    "        return re.sub('[0-9]+', '', content)\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_numericals(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation list:\n",
      "------------------\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                       nice night trip uk delhi üòçüòçüòç  \n",
       "1    may soul rip nation family fellow uk citizens ...\n",
       "2      ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¶‡•Å‡§ñ‡§¶ ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§∂‡•ç‡§∞‡•Ä ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â...\n",
       "3                                    firki queen üò≠  sc\n",
       "4      called great britain looted  trillion usd in...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Removing punctuations\n",
    "PUNCTUATIONS = string.punctuation\n",
    "print('Punctuation list:')\n",
    "print('------------------')\n",
    "print(PUNCTUATIONS)\n",
    "\n",
    "def remove_punctuations(content):\n",
    "    return str(content).translate(str.maketrans('', '', PUNCTUATIONS))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_punctuations(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             nice night trip uk delhi\n",
       "1    may soul rip nation family fellow uk citizens ...\n",
       "2    horrendous shame state acts happenstate govern...\n",
       "3                                       firki queen sc\n",
       "4       called great britain looted trillion usd india\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Remove unwanted emojis\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: clean(str(content), no_emoji=True))\n",
    "df['content'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [nice, night, trip, uk, delhi]\n",
       "1    [may, soul, rip, nation, family, fellow, uk, c...\n",
       "2    [horrendous, shame, state, acts, happenstate, ...\n",
       "3                                   [firki, queen, sc]\n",
       "4    [called, great, britain, looted, trillion, usd...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Tokenizing the texts\n",
    "df['content'] = df['content'].apply(lambda content: word_tokenize(str(content)))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [nice, night, trip, uk, delhi]\n",
       "1    [may, soul, rip, nation, family, fellow, uk, c...\n",
       "2    [horrendous, shame, state, acts, happenstate, ...\n",
       "3                                   [firki, queen, sc]\n",
       "4    [called, great, britain, looted, trillion, usd...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Stemming of the words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stemming_content(content):\n",
    "    text = [stemmer.stem(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content']= df['content'].apply(lambda content: stemming_content(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [nice, night, trip, uk, delhi]\n",
       "1    [may, soul, rip, nation, family, fellow, uk, c...\n",
       "2    [horrendous, shame, state, acts, happenstate, ...\n",
       "3                                   [firki, queen, sc]\n",
       "4    [called, great, britain, looted, trillion, usd...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Lemmatizing the tokens\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "def lemmatizing_content(content):\n",
    "    text = [lemmatizer.lemmatize(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: lemmatizing_content(content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing pre-processed data into csv\n",
    "output_file_path = 'd:\\\\Varshini\\\\CourseWork\\\\Machine Learning\\\\project\\\\MachineLearning-group-project\\\\Preprocessing'\n",
    "df.to_csv('tweets_india_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('mlenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1306746bd56c4df6af1e05061875d5a1c518f29945151cd9f83e4556169c61d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
