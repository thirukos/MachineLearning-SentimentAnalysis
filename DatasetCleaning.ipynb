{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import glob\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base path\n",
    "base_path  = os.getcwd()\n",
    "\n",
    "# Set file path\n",
    "input_file_path = base_path + '\\\\dataset\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all countries tweet\n",
    "joined_files = os.path.join(input_file_path, \"tweets_*.csv\")\n",
    "\n",
    "joined_list = glob.glob(joined_files)\n",
    "\n",
    "# Join the files\n",
    "df = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "print(df)\n",
    "\n",
    "# Write combined tweets onto a new file\n",
    "df.to_csv(input_file_path + '\\\\' + 'all_tweets.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined data set\n",
    "df = pd.read_csv(input_file_path + 'all_tweets.csv')\n",
    "df.columns = ['id', 'date', 'content']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------')\n",
    "print('Dataset content')\n",
    "print('-------------------')\n",
    "print(df.head())\n",
    "print('-------------------')\n",
    "# Some information about the data set\n",
    "print('Length of data set: ', len(df))\n",
    "print('Shape of data set: ', df.shape)\n",
    "\n",
    "print('-------------------')\n",
    "print('Dataset information')\n",
    "print('-------------------')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values - needs to be zero\n",
    "np.sum(df.isnull().any(axis=1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Converting all text to lower case\n",
    "def convert_to_lower_case(data_set):\n",
    "    data_set['content'] = data_set['content'].str.lower()\n",
    "\n",
    "convert_to_lower_case(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removing stop words (un-necessary words) - using nltk's pre-defined stop words\n",
    "# STOP_WORDS = set(stopwords.words('english'))\n",
    "# print('Stop word list:')\n",
    "# print('----------------')\n",
    "# print(STOP_WORDS)\n",
    "\n",
    "# def remove_stop_words(content):\n",
    "#    return \" \".join([text for text in str(content).split() if text not in STOP_WORDS])\n",
    "\n",
    "# df['content'] = df['content'].apply(lambda content: remove_stop_words(content=content))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Removing URLs\n",
    "def remove_URLS(content):\n",
    "    return re.sub('((www.\\S+)|(http[s]?://\\S+))',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_URLS(content=content))\n",
    "#df['content'].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Removing @mentions\n",
    "def remove_mentions(content):\n",
    "    return re.sub('(@\\S+)',' ', str(content))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_mentions(content=content))\n",
    "#df['content'].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Removing numbers\n",
    "def remove_numericals(content):\n",
    "        return re.sub('[0-9]+', '', content)\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_numericals(content=content))\n",
    "#df['content'].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Removing punctuations\n",
    "PUNCTUATIONS = string.punctuation\n",
    "print('Punctuation list:')\n",
    "print('------------------')\n",
    "print(PUNCTUATIONS)\n",
    "\n",
    "def remove_punctuations(content):\n",
    "    return str(content).translate(str.maketrans('', '', PUNCTUATIONS))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_punctuations(content=content))\n",
    "# df['content'].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. convert emoji to text\n",
    "def emoji_convert(content):\n",
    "    return kp_all_emoji_emoticons.replace_keywords(content)\n",
    "\n",
    "from emot.emo_unicode import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS, EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "all_emoji_emoticons = {**EMOTICONS_EMO,**UNICODE_EMOJI_ALIAS, **UNICODE_EMOJI_ALIAS}\n",
    "all_emoji_emoticons = {k:v.replace(\":\",\"\").replace(\"_\",\" \").strip() for k,v in all_emoji_emoticons.items()}\n",
    "kp_all_emoji_emoticons = KeywordProcessor()\n",
    "for k,v in all_emoji_emoticons.items():\n",
    "    kp_all_emoji_emoticons.add_keyword(k, v)\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: emoji_convert(content=content))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.colA.map(lambda x: x.isascii())]\n",
    "\n",
    "df1=df.copy()\n",
    "# df1['DB_user'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "# df1['new'] = df1['content'].apply(lambda content: content.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "# df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Removing 2 letter random words, as it gives no meaning\n",
    "def remove_2letter_words(content):\n",
    "    w = re.sub(r'\\b\\w{1,2}\\b', '', content)\n",
    "    return re.sub(' +', ' ', w)\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_2letter_words(content=content))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Remove unicodes\n",
    "def remove_unicodes(content):\n",
    "        return str(content).encode('ascii', errors='ignore').decode()\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_unicodes(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Remove unwanted emojis\n",
    "df['content'] = df['content'].apply(lambda content: clean(str(content), no_emoji=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Removing meaningless words\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "df['nonEnglish'] = df['content'].apply(lambda x: x.isascii() if isinstance(x, str) else False)\n",
    "df = df[df['nonEnglish'] == True]\n",
    "df = df.drop(['nonEnglish'], axis=1)\n",
    "df['content'] = df['content'].apply(lambda content: remove_2letter_words(content=content))\n",
    "# df['content'] = df['content'].apply(lambda content: remove_stop_words(content=content))\n",
    "df['content'] = df['content'].apply(lambda content: remove_punctuations(content=content))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming of the words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stemming_content(content):\n",
    "    text = [stemmer.stem(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content']= df['content'].apply(lambda content: stemming_content(content=content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the tokens\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "def lemmatizing_content(content):\n",
    "    text = [lemmatizer.lemmatize(word) for word in content]\n",
    "    return content\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: lemmatizing_content(content))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base path\n",
    "base_path  = os.getcwd()\n",
    "\n",
    "# Set file path\n",
    "input_file_path = base_path + '\\\\CleanedDatasets\\\\'\n",
    "\n",
    "df.to_csv(input_file_path + '\\\\' + 'tweets_cleaned.csv', mode='a', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('mlenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1306746bd56c4df6af1e05061875d5a1c518f29945151cd9f83e4556169c61d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
